# DeepLearningPortfolio
DeepLearningPortfolio is a comprehensive collection of Jupyter notebooks showcasing my deep learning projects and experiments. It includes implementations and explorations of advanced image augmentation techniques, mixup methods like Input Mixup, CutMix, Manifold Mixup, as well as analyses of model robustness and calibration.

## 1. Softmax Loss
The "Softmax Loss" directory in this repository contains two Jupyter notebooks that focus on the implementation and application of the softmax loss function in neural networks. Here's a brief overview of each notebook:

### 1.1. softmax.ipynb (Softmax Loss and SGD)
This notebook is dedicated to the implementation and experimentation with the softmax loss function. Key highlights include:

- **Detailed Implementation:** Step-by-step implementation of the softmax loss function, ensuring a clear understanding of its mechanics.
- **Numerical Stability Considerations:** Addresses common numerical issues associated with softmax computations.
- **Stochastic Gradient Descent (SGD):** Integration of the softmax loss function with SGD, demonstrating how it is used for optimizing a model.
- **Testing and Validation:** The notebook includes various tests to validate the correctness and efficiency of the implemented softmax loss function.

### 1.2. two_layer_net.ipynb (Two-Layer MLP Network with Softmax Loss)
This notebook expands on the softmax loss function by integrating it into a two-layer Multi-Layer Perceptron (MLP) network. It includes:

- **Network Architecture:** Implementation of a two-layer fully-connected neural network with ReLU activation.
- **Softmax Loss Integration:** Utilization of the softmax loss function from the previous notebook for network training.
- **Hyperparameter Tuning:** Exploration of different hyperparameters to optimize the network's performance.
- **Visualization and Analysis:** The notebook provides visualizations and in-depth analysis of the network's learning process.

## 2. Convolutional Neural Networks (CNNs, ConvNets)

The "Convolutional Neural Networks (CNNs, ConvNets)" directory in this repository focuses on the implementation and application of CNNs using the PyTorch framework. This directory is an essential resource for understanding the fundamentals and advanced concepts of CNNs. It contains the following Jupyter notebook:

### 2.1. PyTorch.ipynb

This notebook is a comprehensive exploration of CNNs using PyTorch, one of the most popular deep learning frameworks. The key aspects covered in this notebook include:

- **Introduction to CNN Architecture:**
Detailed explanation of the CNN architecture, including convolution layers, pooling layers, and fully-connected layers.

- **PyTorch Framework Usage:**
Step-by-step guide on building CNN models in PyTorch, showcasing the framework's flexibility and ease of use.

- **Image Classification Tasks:**
Implementation of CNNs for image classification tasks, demonstrating the practical application of these networks.

- **Experimentation with CNN Layers:**
Exploration of various layers and techniques used in CNNs, such as different types of convolution and normalization techniques.

- **Hyperparameter Tuning:**
Insights into tuning hyperparameters to enhance model performance, accompanied by practical examples.

- **Visualizations:**
Includes visualizations of training progress, feature maps, and model predictions to offer a clear understanding of how CNNs process and interpret image data.

## 3. Variational Autoencoder (VAE)

The "Variational Autoencoder (VAE)" directory of this repository is dedicated to the exploration and implementation of Variational Autoencoders, a class of generative models. This directory includes the following Jupyter notebook:

### 3.1. VAE.ipynb

This notebook serves as a deep dive into the world of Variational Autoencoders, utilizing them for generative tasks. Key features of this notebook include:

- **Fundamentals of VAEs:**
A comprehensive introduction to the concepts and mathematical foundations underlying Variational Autoencoders.

- **Implementation in Python:**
Step-by-step implementation of a VAE, showcasing how these models are built from scratch.

- **Training and Testing:**
Detailed explanation of the training process, including the optimization of the variational lower bound.

- **Generative Modeling:**
Demonstrations of VAEs in generating new data, highlighting their capability as generative models.

- **Visualization of Results:**
Includes visualizations of the latent space and the generated samples to provide insights into the model's performance and capabilities.

- **Applications:**
Exploration of various applications of VAEs, such as image generation and reconstruction, showcasing their versatility.